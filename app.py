# 10_streamlit_web_interface.py

import os
import sys
import json
import pickle
import pandas as pd
import numpy as np
import faiss
import streamlit as st
from langchain.schema import Document
from anthropic import Anthropic
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel
import torch

print("10ë‹¨ê³„: Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ì‹œì‘")

# í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© í´ë˜ìŠ¤ ì •ì˜
class HybridEmbeddings:
    def __init__(self):
        print("í•˜ì´ë¸Œë¦¬ë“œ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ë‹¤êµ­ì–´ BERT ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)
        self.bert_model_name = "klue/bert-base"
        self.bert_tokenizer = AutoTokenizer.from_pretrained(self.bert_model_name)
        self.bert_model = AutoModel.from_pretrained(self.bert_model_name)
        print(f"BERT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.bert_model_name}")
        
        # SentenceTransformer ëª¨ë¸ (ì˜ë¯¸ì  ì„ë² ë”©)
        self.st_model_name = "jhgan/ko-sroberta-multitask"
        self.st_model = SentenceTransformer(self.st_model_name)
        print(f"SentenceTransformer ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.st_model_name}")
        
        # ì„¤ì •
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.bert_model.to(self.device)
        print(f"ëª¨ë¸ì´ {self.device}ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    
    def get_bert_embeddings(self, texts):
        encoded_input = self.bert_tokenizer(texts, padding=True, truncation=True, 
                                            max_length=512, return_tensors='pt').to(self.device)
        
        with torch.no_grad():
            model_output = self.bert_model(**encoded_input)
        
        sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])
        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)
        
        return sentence_embeddings.cpu().numpy()
    
    def get_st_embeddings(self, texts):
        embeddings = self.st_model.encode(texts, convert_to_tensor=True)
        return embeddings.cpu().numpy()
    
    def embed_documents(self, texts):
        if not texts:
            return []
        
        batch_size = 16
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            bert_embeddings = self.get_bert_embeddings(batch_texts)
            st_embeddings = self.get_st_embeddings(batch_texts)
            
            hybrid_embeddings = np.concatenate([bert_embeddings, st_embeddings], axis=1)
            hybrid_embeddings = hybrid_embeddings / np.linalg.norm(hybrid_embeddings, axis=1, keepdims=True)
            all_embeddings.extend(hybrid_embeddings.tolist())
            
        return all_embeddings
    
    def embed_query(self, text):
        return self.embed_documents([text])[0]

# ì •ìƒë²”ìœ„ ë°ì´í„° í•¨ìˆ˜
def get_normal_ranges():
    """
    í•œêµ­ì¸ ê¸°ì¤€ í˜ˆì•¡ê²€ì‚¬ ì •ìƒë²”ìœ„ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    normal_ranges = {
        "ë°±í˜ˆêµ¬": {"min": 4000, "max": 10000, "unit": "cells/Î¼L"},
        "ì í˜ˆêµ¬": {"min": 4.5, "max": 5.9, "unit": "million cells/Î¼L"},
        "í˜ˆìƒ‰ì†Œ": {"min": 13.5, "max": 17.5, "unit": "g/dL"},
        "í—¤ë§ˆí† í¬ë¦¿": {"min": 41, "max": 53, "unit": "%"},
        "í˜ˆì†ŒíŒ": {"min": 150000, "max": 450000, "unit": "cells/Î¼L"},
        "AST": {"min": 0, "max": 40, "unit": "U/L"},
        "ALT": {"min": 0, "max": 40, "unit": "U/L"},
        "GGT": {"min": 0, "max": 60, "unit": "U/L"},
        "ì´ ë¹Œë¦¬ë£¨ë¹ˆ": {"min": 0.2, "max": 1.2, "unit": "mg/dL"},
        "ì•Œë¶€ë¯¼": {"min": 3.5, "max": 5.2, "unit": "g/dL"},
        "í¬ë ˆì•„í‹°ë‹Œ": {"min": 0.7, "max": 1.3, "unit": "mg/dL"},
        "BUN": {"min": 8, "max": 20, "unit": "mg/dL"},
        "ì´ ì½œë ˆìŠ¤í…Œë¡¤": {"min": 130, "max": 200, "unit": "mg/dL"},
        "HDL": {"min": 40, "max": 60, "unit": "mg/dL"},
        "LDL": {"min": 0, "max": 130, "unit": "mg/dL"},
        "ì¤‘ì„±ì§€ë°©": {"min": 0, "max": 150, "unit": "mg/dL"},
        "ê³µë³µí˜ˆë‹¹": {"min": 70, "max": 99, "unit": "mg/dL"},
        "HbA1c": {"min": 4.0, "max": 5.6, "unit": "%"}
    }
    return normal_ranges

# RAG ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤
class BloodTestRAGSearchEngine:
    def __init__(self, vector_db=None, processed_data_path=None):
        print("RAG ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        self.hybrid_embeddings = HybridEmbeddings()
        self.vector_db = vector_db
        
        # ì •ìƒë²”ìœ„ ë°ì´í„° ë¡œë“œ
        self.normal_ranges = get_normal_ranges()
        
        # ì›ë³¸ ë°ì´í„° ë¡œë“œ (ì„ íƒì )
        if processed_data_path:
            # íŒŒì¼ í™•ì¥ì í™•ì¸
            file_extension = processed_data_path.split('.')[-1].lower()
            if file_extension == 'csv':
                self.processed_data = pd.read_csv(processed_data_path)
            elif file_extension in ['xlsx', 'xls']:
                self.processed_data = pd.read_excel(processed_data_path)
        else:
            self.processed_data = None
            
        print("RAG ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def search(self, query, k=5):
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ì— ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ kê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        """
        print(f"ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘: {query}")
        
        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.hybrid_embeddings.embed_query(query)
        
        # 7ë‹¨ê³„ì—ì„œ ìƒì„±í•œ ë²¡í„° DBê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš° ëŒ€ì‘
        if isinstance(self.vector_db, dict):
            # ë²¡í„° DBê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì§ì ‘ FAISS ê²€ìƒ‰ ìˆ˜í–‰
            query_vector = np.array([query_embedding]).astype('float32')
            faiss.normalize_L2(query_vector)  # ì •ê·œí™”
            
            # FAISS ì¸ë±ìŠ¤ë¡œ ê²€ìƒ‰
            distances, indices = self.vector_db['index'].search(query_vector, k)
            
            results = []
            for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):
                if idx < len(self.vector_db['documents']):
                    doc = self.vector_db['documents'][idx]
                    # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)
                    similarity_score = 1.0 - float(distance) / 2.0
                    
                    result = {
                        'content': doc.page_content,
                        'metadata': doc.metadata,
                        'similarity_score': similarity_score
                    }
                    results.append(result)
            
            return results
        else:
            # ë²¡í„° DBê°€ ì—†ëŠ” ê²½ìš° ì„ì‹œ ê²°ê³¼ ìƒì„±
            print("ë²¡í„° DBê°€ ì—†ê±°ë‚˜ í˜¸í™˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„ì‹œ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # ì„ì‹œ ê²°ê³¼ ìƒì„±
            results = []
            for i in range(3):
                result = {
                    'content': f"ì„ì‹œ ê²€ìƒ‰ ê²°ê³¼ {i+1}",
                    'metadata': {
                        'ì§„ë‹¨ëª…': 'ë¹ˆí˜ˆì¦' if i == 0 else f'ì§„ë‹¨ëª…{i+1}',
                        'ì§ˆë³‘ì½”ë“œ': 'D50' if i == 0 else f'ì½”ë“œ{i+1}'
                    },
                    'similarity_score': 0.9 - (i * 0.1)
                }
                results.append(result)
            return results
    
    def parse_query_results(self, query):
        """
        ì¿¼ë¦¬ ë¬¸ìì—´ì—ì„œ í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ê°’ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        print("ì¿¼ë¦¬ì—ì„œ í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ì¶”ì¶œ ì¤‘...")
        
        # ê²°ê³¼ê°’ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
        parsed_results = {}
        
        # êµ¬ë¶„ìë¡œ ë¶„ë¦¬
        items = query.split(',')
        
        for item in items:
            item = item.strip()
            
            # ê²€ì‚¬í•­ëª©ê³¼ ê²°ê³¼ê°’ ë¶„ë¦¬
            if ':' in item:
                key, value = item.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                # ìˆ«ìë§Œ ì¶”ì¶œ
                try:
                    # ë‹¨ìœ„ê°€ í¬í•¨ëœ ê²½ìš° ìˆ«ìë§Œ ì¶”ì¶œ
                    numeric_value = ''.join(c for c in value if c.isdigit() or c == '.')
                    parsed_results[key] = float(numeric_value)
                except:
                    parsed_results[key] = value
            else:
                # "ê²€ì‚¬í•­ëª© ê°’" í˜•íƒœë¡œ ì…ë ¥ëœ ê²½ìš° ë¶„ë¦¬ ì‹œë„
                parts = item.split()
                if len(parts) == 2:
                    key = parts[0].strip()
                    value = parts[1].strip()
                    
                    try:
                        parsed_results[key] = float(value)
                    except:
                        parsed_results[key] = value
        
        return parsed_results
    
    def analyze_test_results(self, test_results):
        """
        í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì •ìƒë²”ìœ„ì™€ ë¹„êµí•©ë‹ˆë‹¤.
        """
        print("í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        
        analysis = {}
        
        for item, value in test_results.items():
            # ì •ìƒë²”ìœ„ ë°ì´í„°ì— ìˆëŠ” í•­ëª©ì¸ì§€ í™•ì¸
            if item in self.normal_ranges:
                normal_range = self.normal_ranges[item]
                
                if isinstance(value, (int, float)):
                    if value < normal_range["min"]:
                        status = "ë‚®ìŒ"
                        deviation = f"{((normal_range['min'] - value) / normal_range['min'] * 100):.1f}% ë‚®ìŒ"
                    elif value > normal_range["max"]:
                        status = "ë†’ìŒ"
                        deviation = f"{((value - normal_range['max']) / normal_range['max'] * 100):.1f}% ë†’ìŒ"
                    else:
                        status = "ì •ìƒ"
                        deviation = "ì •ìƒë²”ìœ„ ë‚´"
                    
                    analysis[item] = {
                        "value": value,
                        "status": status,
                        "deviation": deviation,
                        "normal_range": f"{normal_range['min']} - {normal_range['max']} {normal_range['unit']}"
                    }
            else:
                # ì •ìƒë²”ìœ„ ë°ì´í„°ì— ì—†ëŠ” í•­ëª©
                if isinstance(value, (int, float)):
                    analysis[item] = {
                        "value": value,
                        "status": "ì •ë³´ì—†ìŒ",
                        "deviation": "ì •ìƒë²”ìœ„ ì •ë³´ ì—†ìŒ",
                        "normal_range": "ì •ë³´ ì—†ìŒ"
                    }
        
        return analysis
    
    def get_possible_diagnoses(self, search_results, test_analysis):
        """
        ê²€ìƒ‰ ê²°ê³¼ì™€ ê²€ì‚¬ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ëŠ¥í•œ ì§„ë‹¨ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        print("ê°€ëŠ¥í•œ ì§„ë‹¨ëª… ì¶”ì¶œ ì¤‘...")
        
        diagnoses = {}
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì§„ë‹¨ëª… ì¶”ì¶œ
        for result in search_results:
            metadata = result['metadata']
            if 'ì§„ë‹¨ëª…' in metadata and metadata['ì§„ë‹¨ëª…'] != 'N/A':
                diagnosis_name = metadata['ì§„ë‹¨ëª…']
                diagnosis_code = metadata['ì§ˆë³‘ì½”ë“œ'] if 'ì§ˆë³‘ì½”ë“œ' in metadata else 'N/A'
                
                if diagnosis_name not in diagnoses:
                    diagnoses[diagnosis_name] = {
                        "code": diagnosis_code,
                        "count": 0,
                        "similarity_scores": [],
                        "matched_patterns": []
                    }
                
                diagnoses[diagnosis_name]["count"] += 1
                diagnoses[diagnosis_name]["similarity_scores"].append(result['similarity_score'])
        
        # í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        for diagnosis in diagnoses.values():
            diagnosis["avg_similarity"] = sum(diagnosis["similarity_scores"]) / len(diagnosis["similarity_scores"])
        
        # ì§„ë‹¨ëª… ì •ë ¬ (ë¹ˆë„ìˆ˜ ë° ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ì¤€)
        sorted_diagnoses = sorted(
            diagnoses.items(),
            key=lambda x: (x[1]["count"], x[1]["avg_similarity"]),
            reverse=True
        )
        
        return sorted_diagnoses

# Claude ì‘ë‹µ ìƒì„±ê¸° í´ë˜ìŠ¤
class ClaudeResponseGenerator:
    def __init__(self, api_key=None):
        """
        Claude API í‚¤ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        print("Claude ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™” ì¤‘...")
        if api_key:
            try:
                self.anthropic = Anthropic(api_key=api_key)
                self.use_api = True
                print("Claude API ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"Claude API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_api = False
        else:
            print("API í‚¤ê°€ ì œê³µë˜ì§€ ì•Šì•„ ëª¨ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
            self.use_api = False
    
    def generate_response(self, query, search_results, test_analysis, possible_diagnoses):
        """
        ì‚¬ìš©ì ì¿¼ë¦¬, ê²€ìƒ‰ ê²°ê³¼, ê²€ì‚¬ ë¶„ì„ ë° ê°€ëŠ¥í•œ ì§„ë‹¨ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ Claudeë¥¼ í†µí•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print("ì‘ë‹µ ìƒì„± ì¤‘...")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompt = f"""
ë‹¹ì‹ ì€ í˜ˆì•¡ê²€ì‚¬ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§„ë‹¨ì„ ë„ì™€ì£¼ëŠ” ì˜ë£Œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™˜ìì˜ í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ê°€ëŠ¥í•œ ì§„ë‹¨ê³¼ ê¶Œì¥ ì‚¬í•­ì„ ì œì‹œí•´ ì£¼ì„¸ìš”.

# ì‚¬ìš©ì ì¿¼ë¦¬
{query}

# í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ë¶„ì„
```
{json.dumps(test_analysis, ensure_ascii=False, indent=2)}
```

# ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì°¾ì€ ìœ ì‚¬ ì‚¬ë¡€ì˜ ìƒìœ„ ì§„ë‹¨ëª… (ë¹ˆë„ìˆœ)
```
{json.dumps(possible_diagnoses[:5] if possible_diagnoses else [], ensure_ascii=False, indent=2)}
```

í™˜ìì˜ í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì‚¬í•­ì„ í¬í•¨í•´ ì£¼ì„¸ìš”:
1. ë¹„ì •ìƒì ì¸ ìˆ˜ì¹˜ì™€ ê·¸ ì˜ë¯¸
2. ê°€ëŠ¥ì„± ë†’ì€ ì§„ë‹¨ëª…ê³¼ ê·¸ ê·¼ê±°
3. ì¶”ê°€ ê²€ì‚¬ê°€ í•„ìš”í•œ í•­ëª©
4. í™˜ìì—ê²Œ ê¶Œì¥í•  ìˆ˜ ìˆëŠ” ì¹˜ë£Œ ë°©ë²•ì´ë‚˜ ìƒí™œ ìŠµê´€ ê°œì„  ì‚¬í•­

ì „ë¬¸ì ì´ê³  ì •í™•í•œ ë¶„ì„ì„ ì œê³µí•˜ë˜, í™˜ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”. ë˜í•œ ë‹¹ì‹ ì€ ì˜ì‚¬ê°€ ì•„ë‹ˆë¯€ë¡œ, ìµœì¢…ì ì¸ ì§„ë‹¨ì€ ì˜ë£Œ ì „ë¬¸ê°€ì˜ íŒë‹¨ì´ í•„ìš”í•¨ì„ ê°•ì¡°í•´ ì£¼ì„¸ìš”.
"""
        
        # API í‚¤ê°€ ìˆëŠ” ê²½ìš° Claude API í˜¸ì¶œ
        if self.use_api:
            try:
                response = self.anthropic.messages.create(
                    model="claude-3-haiku-20240307",
                    max_tokens=1000,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.content[0].text
            except Exception as e:
                print(f"Claude API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return self._generate_mock_response(test_analysis, possible_diagnoses)
        else:
            # API í‚¤ê°€ ì—†ëŠ” ê²½ìš° ëª¨ì˜ ì‘ë‹µ ìƒì„±
            return self._generate_mock_response(test_analysis, possible_diagnoses)
    
    def _generate_mock_response(self, test_analysis, possible_diagnoses):
        """
        API í‚¤ê°€ ì—†ê±°ë‚˜ API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ëª¨ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        abnormal_items = []
        
        # ë¹„ì •ìƒ ìˆ˜ì¹˜ ì°¾ê¸°
        for item, analysis in test_analysis.items():
            if analysis["status"] != "ì •ìƒ":
                abnormal_items.append(f"{item} ({analysis['value']}): {analysis['status']}, {analysis['deviation']}")
        
        # ê°€ëŠ¥í•œ ì§„ë‹¨ëª…
        diagnoses = []
        if possible_diagnoses:
            for i, (diagnosis, info) in enumerate(possible_diagnoses[:3]):
                diagnoses.append(f"{diagnosis} (ìœ ì‚¬ë„: {info['avg_similarity']:.2f})")
        
        # ëª¨ì˜ ì‘ë‹µ ìƒì„±
        response = """
# í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ë¶„ì„

## ë¹„ì •ìƒì ì¸ ìˆ˜ì¹˜ì™€ ì˜ë¯¸
"""
        
        if abnormal_items:
            for item in abnormal_items:
                response += f"- {item}\n"
        else:
            response += "- ëª¨ë“  ê²€ì‚¬ í•­ëª©ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.\n"
            
        response += """
## ê°€ëŠ¥ì„± ë†’ì€ ì§„ë‹¨ëª…
"""
        
        if diagnoses:
            for diagnosis in diagnoses:
                response += f"- {diagnosis}\n"
        else:
            response += "- ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ëšœë ·í•œ ì§„ë‹¨ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
            
        response += """
## ì¶”ê°€ ê²€ì‚¬ ê¶Œì¥ í•­ëª©
- ì •í™•í•œ ì§„ë‹¨ì„ ìœ„í•´ ì¶”ê°€ ê²€ì‚¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì˜ì‚¬ì™€ ìƒë‹´í•˜ì—¬ ì ì ˆí•œ ì¶”ê°€ ê²€ì‚¬ë¥¼ ê²°ì •í•˜ì„¸ìš”.

## ìƒí™œ ìŠµê´€ ê°œì„  ê¶Œì¥ ì‚¬í•­
- ê· í˜• ì¡íŒ ì‹ë‹¨ ìœ ì§€
- ê·œì¹™ì ì¸ ìš´ë™
- ì¶©ë¶„í•œ ìˆ˜ë©´
- ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬

**ì£¼ì˜: ì´ ë¶„ì„ì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í™œìš©í•˜ì‹œê³ , ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œë¥¼ ìœ„í•´ ë°˜ë“œì‹œ ì˜ë£Œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì…”ì•¼ í•©ë‹ˆë‹¤.**
"""
        
        return response

class BloodTestRAGPipeline:
    def __init__(self, search_engine, response_generator):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        """
        print("RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        self.search_engine = search_engine
        self.response_generator = response_generator
    
    def process_query(self, query):
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        print(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {query}")
        
        # 1. ì¿¼ë¦¬ì—ì„œ í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ì¶”ì¶œ
        # í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ê°€ í¬í•¨ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        test_results_part = query
        if "í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼:" in query:
            test_results_part = query.split("í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼:")[1].strip()
        
        test_results = self.search_engine.parse_query_results(test_results_part)
        print(f"ì¶”ì¶œëœ ê²€ì‚¬ ê²°ê³¼: {test_results}")
        
        # 2. ê²€ì‚¬ ê²°ê³¼ ë¶„ì„
        test_analysis = self.search_engine.analyze_test_results(test_results)
        
        # 3. ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.search_engine.search(query)
        
        # 4. ê°€ëŠ¥í•œ ì§„ë‹¨ëª… ì¶”ì¶œ
        possible_diagnoses = self.search_engine.get_possible_diagnoses(search_results, test_analysis)
        
        # 5. ì‘ë‹µ ìƒì„±
        response = self.response_generator.generate_response(
            query, search_results, test_analysis, possible_diagnoses
        )
        
        # 6. ê²°ê³¼ ë°˜í™˜
        result = {
            "query": query,
            "test_results": test_results,
            "test_analysis": test_analysis,
            "possible_diagnoses": possible_diagnoses,
            "response": response
        }
        
        return result

# Google Driveì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ ì •ì˜
def download_file_from_google_drive(file_id, destination):
    import requests
    
    def get_confirm_token(response):
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                return value
        return None

    def save_response_content(response, destination):
        CHUNK_SIZE = 32768
        with open(destination, "wb") as f:
            for chunk in response.iter_content(CHUNK_SIZE):
                if chunk:  # filter out keep-alive new chunks
                    f.write(chunk)

    URL = "https://docs.google.com/uc?export=download"
    session = requests.Session()

    response = session.get(URL, params={'id': file_id}, stream=True)
    token = get_confirm_token(response)

    if token:
        params = {'id': file_id, 'confirm': token}
        response = session.get(URL, params=params, stream=True)

    save_response_content(response, destination)

# RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì „ì—­ ë³€ìˆ˜)
@st.cache_resource
def initialize_rag_pipeline():
    import tempfile
    import os
    import requests
    
    # API í‚¤ë¥¼ Streamlit secretsì—ì„œ ê°€ì ¸ì˜´
    try:
        claude_api_key = st.secrets["anthropic"]["api_key"]
    except Exception as e:
        st.warning("API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ì˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")
        claude_api_key = None
    
    # í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    try:
        with st.spinner("ë²¡í„° DB ë‹¤ìš´ë¡œë“œ ì¤‘..."):
            # Google Drive íŒŒì¼ ID
            vector_db_file_id = "1K0_7pDzfawEnllbtXFeZuOa5JgPaYv3h"
            
            # ì„ì‹œ íŒŒì¼ ê²½ë¡œ
            vector_db_path = os.path.join(tempfile.gettempdir(), "vector_db.pkl")

            # GitHub ì €ì¥ì†Œì˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            current_dir = os.path.dirname(os.path.abspath(__file__))
            data_path = os.path.join(current_dir, "final_data.csv")

            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ë¥¼ ì§ì ‘ êµ¬í˜„
            def download_from_drive(file_id, destination):
                URL = "https://docs.google.com/uc?export=download"
                session = requests.Session()

                response = session.get(URL, params={'id': file_id}, stream=True)
                token = None
                for key, value in response.cookies.items():
                    if key.startswith('download_warning'):
                        token = value
                        break

                if token:
                    params = {'id': file_id, 'confirm': token}
                    response = session.get(URL, params=params, stream=True)

                CHUNK_SIZE = 32768
                with open(destination, "wb") as f:
                    for chunk in response.iter_content(CHUNK_SIZE):
                        if chunk:  # filter out keep-alive new chunks
                            f.write(chunk)
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            download_file_from_google_drive(vector_db_file_id, vector_db_path)
            
            st.success("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            
            # ë²¡í„° DB ë¡œë“œ
            with open(vector_db_path, 'rb') as f:
                vector_db = pickle.load(f)
        
        # RAG ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        search_engine = BloodTestRAGSearchEngine(vector_db, data_path)
        
        # Claude ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”
        response_generator = ClaudeResponseGenerator(claude_api_key)
        
        # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        rag_pipeline = BloodTestRAGPipeline(search_engine, response_generator)
        
        return rag_pipeline
        
    except Exception as e:
        import traceback
        st.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰
        st.warning("ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        search_engine = BloodTestRAGSearchEngine(None, None)
        response_generator = ClaudeResponseGenerator(claude_api_key)
        return BloodTestRAGPipeline(search_engine, response_generator)

# Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
def main():
    st.set_page_config(
        page_title="í˜ˆì•¡ê²€ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ",
        page_icon="ğŸ©¸",
        layout="wide"
    )
    
    st.title("í˜ˆì•¡ê²€ì‚¬ ë¶„ì„ ì‹œìŠ¤í…œ")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    rag_pipeline = initialize_rag_pipeline()
    
    # ì •ìƒë²”ìœ„ ë°ì´í„° ë¡œë“œ
    normal_ranges = get_normal_ranges()
    test_items = sorted(normal_ranges.keys())
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'test_results' not in st.session_state:
        st.session_state.test_results = {}
    
    # ë ˆì´ì•„ì›ƒ (2ê°œ ì»¬ëŸ¼)
    col1, col2 = st.columns(2)
    
    # ì…ë ¥ í¼ (ì™¼ìª½ ì»¬ëŸ¼)
    with col1:
        st.header("í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼ ì…ë ¥")
        
        with st.form("test_form"):
            selected_item = st.selectbox("ê²€ì‚¬ í•­ëª© ì„ íƒ", [""] + test_items)
            value = st.number_input("ê²€ì‚¬ ê²°ê³¼ ê°’", step=0.01)
            
            submitted = st.form_submit_button("í•­ëª© ì¶”ê°€")
            if submitted and selected_item and value:
                st.session_state.test_results[selected_item] = float(value)
                st.success(f"{selected_item}: {value} í•­ëª©ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì¶”ê°€ëœ í•­ëª© í‘œì‹œ
        if st.session_state.test_results:
            st.subheader("ì¶”ê°€ëœ ê²€ì‚¬ í•­ëª©")
            
            for item, value in st.session_state.test_results.items():
                col_a, col_b = st.columns([5, 1])
                with col_a:
                    st.write(f"**{item}**: {value}")
                with col_b:
                    if st.button("ì‚­ì œ", key=f"del_{item}"):
                        del st.session_state.test_results[item]
                        st.experimental_rerun()
        
        # ë¶„ì„ ë° ì´ˆê¸°í™” ë²„íŠ¼
        col_analyze, col_reset = st.columns(2)
        
        with col_analyze:
            analyze_btn = st.button("ë¶„ì„í•˜ê¸°", disabled=len(st.session_state.test_results) == 0)
        
        with col_reset:
            reset_btn = st.button("ì´ˆê¸°í™”")
            if reset_btn:
                st.session_state.test_results = {}
                st.experimental_rerun()
    
    # ê²°ê³¼ í‘œì‹œ (ì˜¤ë¥¸ìª½ ì»¬ëŸ¼)
    with col2:
        st.header("ë¶„ì„ ê²°ê³¼")
        
        if analyze_btn and st.session_state.test_results:
            with st.spinner("ë¶„ì„ ì¤‘..."):
                # ì¿¼ë¦¬ ìƒì„±
                query_parts = ["í˜ˆì•¡ê²€ì‚¬ ê²°ê³¼:"]
                for item, val in st.session_state.test_results.items():
                    query_parts.append(f"{item}: {val}")
                
                query = ", ".join(query_parts)
                
                # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                if rag_pipeline:
                    result = rag_pipeline.process_query(query)
                    
                    # ë¹„ì •ìƒ í•­ëª© í‘œì‹œ
                    st.subheader("ë¹„ì •ìƒ ìˆ˜ì¹˜ í•­ëª©")
                    
                    analysis = result["test_analysis"]
                    abnormal_items = [(item, data) for item, data in analysis.items() if data["status"] != "ì •ìƒ"]
                    
                    if abnormal_items:
                        for item, data in abnormal_items:
                            if data["status"] == "ë†’ìŒ":
                                color = "danger"
                            elif data["status"] == "ë‚®ìŒ":
                                color = "primary"
                            else:
                                color = "secondary"
                            
                            st.markdown(f"""
                            <div style='padding: 10px; border-radius: 5px; background-color: {'#f8d7da' if color == 'danger' else '#cfe2ff' if color == 'primary' else '#e2e3e5'}'>
                                <b>{item}:</b> {data['value']} ({data['status']})<br>
                                <small>ì •ìƒ ë²”ìœ„: {data['normal_range']}</small><br>
                                <small>{data['deviation']}</small>
                            </div>
                            """, unsafe_allow_html=True)
                    else:
                        st.success("ëª¨ë“  ê²€ì‚¬ í•­ëª©ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
                    
                    # ì§„ë‹¨ëª… í‘œì‹œ
                    st.subheader("ê°€ëŠ¥ì„± ë†’ì€ ì§„ë‹¨ëª…")
                    
                    if result["possible_diagnoses"]:
                        for diagnosis, info in result["possible_diagnoses"][:5]:
                            st.write(f"**{diagnosis}** (ì½”ë“œ: {info['code']})")
                            st.write(f"ìœ ì‚¬ë„: {info['avg_similarity']:.2f}")
                            st.write("---")
                    else:
                        st.info("ìœ ì‚¬í•œ ì§„ë‹¨ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    # ìƒì„¸ ë¶„ì„ ë° ê¶Œì¥ ì‚¬í•­
                    st.subheader("ìƒì„¸ ë¶„ì„ ë° ê¶Œì¥ ì‚¬í•­")
                    st.markdown(result["response"])
                else:
                    st.error("RAG íŒŒì´í”„ë¼ì¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
